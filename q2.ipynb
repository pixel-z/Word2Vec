{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "q2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re"
      ],
      "outputs": [],
      "metadata": {
        "id": "yi_FMHo41cXt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!wget http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_5.json.gz"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-17 22:21:38--  http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_5.json.gz\n",
            "Resolving snap.stanford.edu (snap.stanford.edu)... 171.64.75.80\n",
            "Connecting to snap.stanford.edu (snap.stanford.edu)|171.64.75.80|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 495854086 (473M) [application/x-gzip]\n",
            "Saving to: ‘reviews_Electronics_5.json.gz.1’\n",
            "\n",
            "reviews_Electronics 100%[===================>] 472.88M  3.54MB/s    in 2m 41s  \n",
            "\n",
            "2021-09-17 22:24:20 (2.94 MB/s) - ‘reviews_Electronics_5.json.gz.1’ saved [495854086/495854086]\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "id": "hLlUv8rI2uic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95648c9c-d310-4046-c705-652388c7dfc7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Reading the data\n",
        "import json\n",
        "import gzip\n",
        "\n",
        "json_content = []\n",
        "file = 'reviews_Electronics_5.json.gz'\n",
        "with gzip.open(file , 'rb') as gzip_file:\n",
        "    for line in gzip_file:\n",
        "\n",
        "        line = line.rstrip()  # removing xtra spaces\n",
        "        if line:\n",
        "            obj = json.loads(line)\n",
        "            json_content.append(obj['reviewText'])\n",
        "            \n",
        "            if len(json_content) == 50000:\n",
        "                break"
      ],
      "outputs": [],
      "metadata": {
        "id": "1wEbtjIS21Rd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2_PqfmTCEQ9",
        "outputId": "af86eda5-ef36-40af-a440-569e301f24a4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# sentences = 1D array of sentence\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "sentences = []\n",
        "for i in json_content:\n",
        "    sentences += sent_tokenize(i)\n",
        "\n",
        "print(sentences[0])\n",
        "# print(word_tokenize(sentences[1][0]))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We got this GPS for my husband who is an (OTR) over the road trucker.\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hIjFz-kLHA9",
        "outputId": "997aebde-194d-4f12-b2d9-cf9fca7d7465"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# removing punctuation\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "# words = 1D array of words\n",
        "words = []\n",
        "for i in sentences:\n",
        "    words.append(\"<S>\")\n",
        "    words += tokenizer.tokenize(i.lower())  # lowercase words\n",
        "    # words.append(\"<E>\")\n",
        "\n",
        "print(words[:25])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<S>', 'we', 'got', 'this', 'gps', 'for', 'my', 'husband', 'who', 'is', 'an', 'otr', 'over', 'the', 'road', 'trucker', '<S>', 'very', 'impressed', 'with', 'the', 'shipping', 'time', 'it', 'arrived']\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yt8MSZlCobd",
        "outputId": "13ee9df2-c52a-42ee-829d-12323c8cc20e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "freq = defaultdict(int)\n",
        "for i in words:\n",
        "  freq[i]+=1\n",
        "\n",
        "# removing words freq < 5\n",
        "new_words = [i for i in words if freq[i] > 5]\n",
        "\n",
        "print(new_words[:25])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<S>', 'we', 'got', 'this', 'gps', 'for', 'my', 'husband', 'who', 'is', 'an', 'over', 'the', 'road', 'trucker', '<S>', 'very', 'impressed', 'with', 'the', 'shipping', 'time', 'it', 'arrived', 'a']\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTNVkCxmVMrH",
        "outputId": "8fd681a5-70ec-41ee-e078-d55e777cb09e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Generating training data\n",
        "\n",
        "# word2ind & ind2word\n",
        "word2ind=defaultdict(list)\n",
        "ind2word=defaultdict(list)\n",
        "\n",
        "c = 0\n",
        "# giving each word its index\n",
        "for i in freq:\n",
        "  if freq[i] > 5:\n",
        "    ind2word[c]=i\n",
        "    word2ind[i]=c\n",
        "    c+=1\n",
        "\n",
        "# print(freq[new_words[word2ind[\"<E>\"]]])\n",
        "# print(new_words[word2ind[\"<E>\"]])"
      ],
      "outputs": [],
      "metadata": {
        "id": "Uiorpn-Vpt48"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import pickle\n",
        "\n",
        "# 1. storing ind2word\n",
        "# embedding = [i for i in ind2word]\n",
        "file = open(\"./drive/MyDrive/NLP/q2/ind2word.pkl\",\"wb\")\n",
        "i2w = { i:ind2word[i] for i in ind2word}\n",
        "pickle.dump(i2w,file)\n",
        "file.close()\n",
        "\n",
        "# 2. storing word2ind\n",
        "# embedding = [i for i in word2ind]\n",
        "file = open(\"./drive/MyDrive/NLP/q2/word2ind.pkl\",\"wb\")\n",
        "w2i = { i:word2ind[i] for i in word2ind}\n",
        "pickle.dump(w2i,file)\n",
        "file.close()\n",
        "\n",
        "# print(word2ind)"
      ],
      "outputs": [],
      "metadata": {
        "id": "PhFP_mwEmNIr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Generating training data\n",
        "\n",
        "# We can uniquely give one hot encoding to the vectors by using their index nos (Word2Ind)\n",
        "# def one_hot_encode(word, V):\n",
        "#   vec = np.zeros(V)\n",
        "#   vec[word2ind[word]] = 1\n",
        "#   return vec\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x)\n",
        "    ycap = e_x/np.sum(e_x, axis=0)\n",
        "    return ycap"
      ],
      "outputs": [],
      "metadata": {
        "id": "dsMMuRcPUlOr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Initializing weight matrices W1 & W2\n",
        "def init_model(N,V):\n",
        "  W1 = np.random.rand(V,N)\n",
        "  W2 = np.random.rand(N,V)\n",
        "  return W1,W2\n",
        "\n",
        "# forward propogation\n",
        "def forward_prop(x, W1, W2):\n",
        "  h=np.dot(x,W1)\n",
        "  u=np.dot(h,W2) \n",
        "  y_c = softmax(u.T)\n",
        "  return y_c.T, h, u\n",
        "\n",
        "# backward propogation\n",
        "def back_prop(x, h, err, batch_size, W2):\n",
        "    gradW1 = np.dot(x.T, np.dot(err, W2.T) ) / batch_size\n",
        "    gradW2 = np.dot(h.T, err) / batch_size\n",
        "    return gradW1, gradW2\n",
        "\n",
        "epoch = -1\n",
        "def get_vectors(batch_size, window):\n",
        "  global epoch\n",
        "  while True:\n",
        "    epoch+=1\n",
        "    for i in range(0,len(new_words)):\n",
        "      if new_words[i] == \"<S>\": continue\n",
        "      \n",
        "      w_context=np.zeros(V)\n",
        "      w_target=np.zeros(V)\n",
        "      w_target[word2ind[new_words[i]]]=1\n",
        "      \n",
        "      mean=0\n",
        "      # back loop\n",
        "      for j in range(1,window+1):\n",
        "        if i-j<0 or new_words[i-j] == \"<S>\": break\n",
        "        w_context[word2ind[new_words[i-j]]]+=1\n",
        "        mean+=1\n",
        "\n",
        "      # forward loop\n",
        "      for j in range(1,window+1):\n",
        "        if i+j>=len(new_words) or new_words[i+j] == \"<S>\": break\n",
        "        w_context[word2ind[new_words[i+j]]]+=1\n",
        "        mean+=1        \n",
        "      \n",
        "      if mean:\n",
        "        w_context/=mean   \n",
        "      yield w_context,w_target\n",
        "\n",
        "def get_batches(batch_size, window):\n",
        "  batch_target=[]\n",
        "  batch_context=[]\n",
        "  for x,y in get_vectors(batch_size, window):\n",
        "    if len(batch_context) < batch_size:\n",
        "      batch_context.append(x)\n",
        "      batch_target.append(y)\n",
        "    else:\n",
        "      yield np.array(batch_context),np.array(batch_target)\n",
        "      batch_target=[]\n",
        "      batch_context=[]\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "0v99hGAoo4xJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J45qnI1Oz7rf",
        "outputId": "a24397bf-9964-42c6-830f-da8645b41440"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# save embeddings periodically\n",
        "import pickle\n",
        "\n",
        "def save_embeddings(W1, W2, ycap, y, h, iterations, eta):\n",
        "  embedding = [i for i in W2]\n",
        "  file = open(\"./drive/MyDrive/NLP/w2.pkl\",\"wb\")\n",
        "  pickle.dump(embedding,file)\n",
        "  file.close()\n",
        "\n",
        "  embedding = [i for i in W1]\n",
        "  file = open(\"./drive/MyDrive/NLP/w1.pkl\",\"wb\")\n",
        "  pickle.dump(embedding,file)\n",
        "  file.close()\n",
        "\n",
        "  embedding = ycap\n",
        "  file = open(\"./drive/MyDrive/NLP/ycap.pkl\",\"wb\")\n",
        "  pickle.dump(embedding,file)\n",
        "  file.close()\n",
        "\n",
        "  embedding = y\n",
        "  file = open(\"./drive/MyDrive/NLP/y.pkl\",\"wb\")\n",
        "  pickle.dump(embedding,file)\n",
        "  file.close()\n",
        "\n",
        "  embedding = h\n",
        "  file = open(\"./drive/MyDrive/NLP/h.pkl\",\"wb\")\n",
        "  pickle.dump(embedding,file)\n",
        "  file.close()\n",
        "\n",
        "  embedding = iterations\n",
        "  file = open(\"./drive/MyDrive/NLP/iterations.pkl\",\"wb\")\n",
        "  pickle.dump(embedding,file)\n",
        "  file.close()\n",
        "\n",
        "  embedding = eta\n",
        "  file = open(\"./drive/MyDrive/NLP/eta.pkl\",\"wb\")\n",
        "  pickle.dump(embedding,file)\n",
        "  file.close()"
      ],
      "outputs": [],
      "metadata": {
        "id": "55FcSsChjKGy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Training\n",
        "\n",
        "# mini batch gradient descent\n",
        "def gradient_descent(batch_size, eta, iterations, N, V, window):\n",
        "  W1,W2 = init_model(N,V)\n",
        "  \n",
        "  for x,y in get_batches(batch_size, window):\n",
        "    iterations-=1\n",
        "\n",
        "    # forward propogation\n",
        "    ycap, h, u = forward_prop(x, W1, W2)\n",
        "\n",
        "    # back propagation\n",
        "    err = ycap - y\n",
        "    gradientW1, gradientW2 = back_prop(x, h, err, batch_size, W2)\n",
        "\n",
        "    W1 -= eta*gradientW1\n",
        "    W2 -= eta*gradientW2\n",
        "\n",
        "    if iterations%100 == 0:\n",
        "      eta *= 0.7\n",
        "    if iterations%1000 == 0:\n",
        "      save_embeddings(W1, W2, ycap, y, h, iterations, eta)\n",
        "    if iterations <= 0:\n",
        "      break\n",
        "  \n",
        "  return W1, W2\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "_E-gQO-dlAKq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# parameters\n",
        "batch_size = 64\n",
        "eta = 0.035 # rate of change (η)\n",
        "N = 100\n",
        "iterations = 150000\n",
        "window = 2\n",
        "V = len(word2ind) # Unique Vocabulary\n",
        "\n",
        "# calling gradient descent\n",
        "W1, W2 = gradient_descent(batch_size, eta, iterations, N, V, window)\n",
        "\n",
        "# print(W1)\n",
        "# print(W2)"
      ],
      "outputs": [],
      "metadata": {
        "id": "WxmQCUREnSww"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# returns top10 nearest neighbours to 'str' string\n",
        "def top10(str):\n",
        "  array = []\n",
        "  v1 = W2.T[word2ind[str]]\n",
        "  for i in range (0,len(W2.T)):\n",
        "    # angle = cosine_similarity(v1, U[i])\n",
        "    v2 = W2.T[i]\n",
        "    angle = np.dot(v1,v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
        "    array.append([angle,ind2word[i]])\n",
        "  \n",
        "  array.sort(reverse=True)\n",
        "  # print(array[:10])\n",
        "  return array[:10]\n",
        "\n",
        "print(top10(\"camera\"))"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slnAGTIDCK0k",
        "outputId": "d17b8e0f-b426-49d7-b6ae-bbc107b6c0c5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(top10(\"awesome\"))"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWn6qeMTBjV-",
        "outputId": "437eb833-5aa2-4b85-9aa6-fdfdc3475c09"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(top10(\"terrible\"))"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZ2hrKjGBryL",
        "outputId": "7c8db61e-1077-4d8e-d260-99885310a3c9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(top10(\"device\"))"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGlEh5rwBsVW",
        "outputId": "832c1d49-da63-4dc1-b65e-215c417092f5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(top10(\"good\"))"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5m0_6qmFBsxz",
        "outputId": "330af1cd-a508-4796-9ae0-f7e899236cb0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(top10(\"hard\"))"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9iWrAnaBtII",
        "outputId": "17347bf3-8c47-419d-eebf-b968f841ac82"
      }
    }
  ]
}